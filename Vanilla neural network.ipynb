{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Vanilla Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # To grab the images and extract useful information\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42) # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the labels and file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4870, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the dataset directory\n",
    "dataset_dir = os.getcwd() + \"/train_selected\"\n",
    "\n",
    "# Get the data labels\n",
    "labels_file = dataset_dir + \"/train_selected.csv\"\n",
    "data_labels = pd.read_csv(labels_file)\n",
    "\n",
    "data_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X files\n",
    "file_list = [dataset_dir + \"/\" + str(x) + \".png\" for x in list(data_labels[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4370\n",
       "1     500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "data_labels[\"class\"] = np.where(data_labels['label']=='automobile', 1, 0)\n",
    "data_labels[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will standardise the dataset\n",
    "# Replace False\n",
    "\n",
    "def standarise_data(dataset):\n",
    "    \n",
    "    new_dataset = dataset/255.\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global X_train, X_test, y_train, y_test, X, y\n",
    "    \n",
    "    X = np.array([np.array(Image.open(fname)) for fname in file_list])\n",
    "    y = np.array(data_labels[\"class\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    y_train = y_train.reshape(1, y_train.shape[0])\n",
    "    y_test = y_test.reshape(1, y_test.shape[0])\n",
    "    \n",
    "    # Reshape the training and test examples \n",
    "    X_train_f = X_train.reshape(X_train.shape[0], -1).T\n",
    "    X_test_f = X_test.reshape(X_test.shape[0], -1).T\n",
    "    \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    X_train = standarise_data(X_train_f)\n",
    "    X_test = standarise_data(X_test_f)\n",
    "    \n",
    "    print (\"Flatten X_train: \" + str(X_train.shape))\n",
    "    print (\"Flatten X_test: \" + str(X_test.shape))\n",
    "    \n",
    "    print (\"y_train: \" + str(y_train.shape))\n",
    "    print (\"y_test: \" + str(y_test.shape))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten X_train: (3072, 3409)\n",
      "Flatten X_test: (3072, 1461)\n",
      "y_train: (1, 3409)\n",
      "y_test: (1, 1461)\n"
     ]
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3409, 3072) (1461, 3072) (3409,) (1461,)\n"
     ]
    }
   ],
   "source": [
    "X_train_clf = X_train.T\n",
    "X_test_clf = X_test.T\n",
    "\n",
    "y_train_clf = y_train.T.ravel()\n",
    "y_test_clf = y_test.T.ravel()\n",
    "\n",
    "print(X_train_clf.shape, X_test_clf.shape, y_train_clf.shape, y_test_clf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done   3 out of  10 | elapsed:  3.9min remaining:  9.1min\n",
      "[Parallel(n_jobs=7)]: Done  10 out of  10 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Total time taken: 0:05:46.853523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "import datetime\n",
    "\n",
    "C_list = np.linspace(0.001, 0.5, 20)\n",
    "log_reg = LogisticRegressionCV(\n",
    "    Cs=C_list, cv=10, penalty='l2', scoring='roc_auc', solver='liblinear', tol =1e-4, max_iter=1000, \n",
    "    class_weight='balanced', n_jobs=7, verbose=2, refit=True, multi_class='ovr', random_state=42\n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "log_reg.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the class\n",
    "y_test_clf = pd.DataFrame(y_test_clf, columns=[\"actual\"])\n",
    "y_test_clf[\"predictions_lr\"] = log_reg.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix \n",
      " [[1154  143]\n",
      " [  53  111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92      1297\n",
      "           1       0.44      0.68      0.53       164\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1461\n",
      "   macro avg       0.70      0.78      0.73      1461\n",
      "weighted avg       0.90      0.87      0.88      1461\n",
      "\n",
      "ROC-AUC Score \n",
      " 0.7832874174925251\n",
      "Accuracy Score \n",
      " 0.865845311430527\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0:00:21.064523\n"
     ]
    }
   ],
   "source": [
    "# A tree based example\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "#Create the model object\n",
    "rf_class = RandomForestClassifier(\n",
    "    n_estimators=1000, criterion='entropy', \n",
    "    max_depth=15, min_samples_split=3, bootstrap=True, oob_score=True, \n",
    "    n_jobs=7, random_state=42, verbose=1, class_weight='balanced' \n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "rf_class.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#Predict the class\n",
    "y_test_clf[\"predictions_rf\"] = rf_class.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix \n",
      " [[1297    0]\n",
      " [ 151   13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94      1297\n",
      "           1       1.00      0.08      0.15       164\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1461\n",
      "   macro avg       0.95      0.54      0.55      1461\n",
      "weighted avg       0.91      0.90      0.86      1461\n",
      "\n",
      "ROC-AUC Score \n",
      " 0.5396341463414634\n",
      "Accuracy Score \n",
      " 0.8966461327857632\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3072, 40, 70, 90, 25, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correctly create the layer dimensions as per the brief\n",
    "# Replace False\n",
    "\n",
    "# For example layer_dimensions of [5,7,2,1] would be input 5, two hidden layers (7,2) and 1 in output\n",
    "# [3072, 10, 25, 10, 1]\n",
    "layer_dimensions = [3072, 40, 70, 90, 25, 1]\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# He, Xavier and 0.01*weight initialisation\n",
    "\n",
    "def initialise_parameters(layer_dimensions, initialization_method):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    layer_dimensions -- python (list), one item per layer, number representing size of layer\n",
    "    initialization_method -- Base, He or Xavier initialisation\n",
    "    \n",
    "    Output:\n",
    "    parameters -- python dictionary containing your weight and bias parameters \"W1\", \"b1\", ..., \"WL\", \"bL\" \n",
    "                  with appropriate sizes.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)               \n",
    "    parameters = {}                 \n",
    "    L = len(layer_dimensions) # Number of layers in the network          \n",
    "\n",
    "    if initialization_method == \"he\": # He initialisation\n",
    "        for l in range(1, L):\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l - 1]) * np.sqrt(2 / layer_dimensions[l - 1])\n",
    "            parameters[\"b\" + str(l)] = np.zeros((layer_dimensions[l], 1))\n",
    "            \n",
    "    elif initialization_method == \"xavier\": # Xavier initialisation\n",
    "        for l in range(1, L):\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l - 1]) * np.sqrt(1 / layer_dimensions[l - 1])\n",
    "            parameters[\"b\" + str(l)] = np.zeros((layer_dimensions[l], 1))\n",
    "            \n",
    "    else:\n",
    "        for l in range(1, L): # 0.01* weight initialisation\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dimensions[l],layer_dimensions[l-1]) * 0.01 \n",
    "            parameters['b' + str(l)] = np.zeros((layer_dimensions[l], 1))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward prop\n",
    "\n",
    "Create a series of functions that will:\n",
    "\n",
    "* Undertake the linear multiplication\n",
    "* Undertake the activation of the layer\n",
    "* Store this somewhere for efficient computation of backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "We will need activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will undertake sigmoid activation\n",
    "# Create another function that will undertake relu activation\n",
    "# Replace False\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of sigmoid(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z)) \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def hyperbolic_tan(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = np.where(Z > 0, Z, Z * 0.01)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will undertake the linear component of forward prop\n",
    "# Replace False\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    A     -- activations from previous layer\n",
    "    W     -- weights matrix\n",
    "    b     -- bias vector\n",
    "\n",
    "    Output:\n",
    "    Z     -- the input to activation function \n",
    "    cache -- a python dictionary with \"A\", \"W\" and \"b\" for backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A)+b # Z is calculated using the dot product of weight and activation value and addition of bias term\n",
    "\n",
    "    cache = (A, W, b) # cache activation, weight and bias values for use in backward propogation\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function conditionally calls an activation function. \n",
    "# Call the correction function above with the correct if statement\n",
    "# Replace False\n",
    "\n",
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "    A_prev     -- activations from previous layer\n",
    "    W          -- weights matrix\n",
    "    b          -- bias vector\n",
    "    activation -- the activation type to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Output:\n",
    "    A          -- the output of the activation function, also called the post-activation value \n",
    "    cache      -- a python dictionary with two two caches \"linear_cache\" and \"activation_cache\" for backprop\n",
    "    \"\"\"    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    ###NOTE###\n",
    "    # This is where you can put more activation functions for the extension tasks\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = hyperbolic_tan(Z)\n",
    "        \n",
    "    elif activation == \"leaky_relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "    \n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architect the forward pass. \n",
    "# You will need to firstly determine how many layers there are\n",
    "# You will then need to pull out the correct parameters we initalised\n",
    "# Ensure you use the appropriate activation for the middle layers\n",
    "# Pay special attention to the last layer\n",
    "# It may help to print out parameters\n",
    "# Replace False\n",
    "\n",
    "def total_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    X            -- raw data\n",
    "    parameters   -- dictionary of initialised parameters, output from a particular function above.\n",
    "    \n",
    "    Returns:\n",
    "    AL           -- last post-activation value\n",
    "    caches       -- list of caches from forward activations\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(layer_dimensions)\n",
    "    \n",
    "    # All the layers up until the last (sigmoid) layer\n",
    "    for l in range(1, L-1):\n",
    "        A_prev = A \n",
    "        A, cache = activation_forward(A_prev, \n",
    "                                      parameters[\"W\"+ str(l)], \n",
    "                                      parameters[\"b\"+ str(l)], \n",
    "                                      activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # The last layer - how do we use the sigmoid function?\n",
    "\n",
    "    AL, cache = activation_forward(A, \n",
    "                                   parameters[\"W\"+ str(L-1)], \n",
    "                                   parameters[\"b\"+ str(L-1)], \n",
    "                                   activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate the relu and the sigmoid functions\n",
    "# Replace False\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in the backwards prop here.\n",
    "\n",
    "    Output:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in backprop here\n",
    "\n",
    "    Returns:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    sig = sigmoid(Z)[0]\n",
    "    dZ = dA * sig * (1-sig)\n",
    "    return dZ\n",
    "\n",
    "def hyperbolic_tan_backward(dA, cache):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = 1 - (hyperbolic_tan(Z)[0] ** 2)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def leaky_relu_backward(dA, cache):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z < 0] = 0.01\n",
    "    dZ[Z > 0] = 1\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to do anything here, but notes are included for your interest\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dZ        -- Gradient of the cost with respect to 'Z' of current layer\n",
    "    cache     -- (A_prev, W, b) from forward propag in the current layer, we stored this previously\n",
    "\n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T) \n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the activation differentiation functions you created above\n",
    "# Ensure you are putting the right arguments (hint: caches) into the functions\n",
    "# For the first false, consider what function give back dZ? (what does it require?)\n",
    "# Replace False\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA         -- post-activation gradient for current layer\n",
    "    cache      -- (linear_cache, activation_cache) stored previously for backprop\n",
    "    activation -- activation for this layer (\"sigmoid\" or \"relu\")\n",
    "    \n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = hyperbolic_tan_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate the loss function with respect to the last activation layer\n",
    "# Replace False\n",
    "\n",
    "def total_backward(AL, Y, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    AL        -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y         -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches    -- list of caches from relu and sigmoid we kept from forward prop\n",
    "    \n",
    "    output:\n",
    "    grads     -- A dictionary with the gradients named dA+l,dW+l, db+l for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the binary logistic cost function ('cross entropy loss')\n",
    "# This is on page 51 of the slides from block_1. \n",
    "# You may need to transpose elements to make the matrix calculations work\n",
    "# Replace False\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    AL    -- probability vector for label predictions\n",
    "    Y     -- truth vector vector\n",
    "\n",
    "    Output:\n",
    "    cost  -- cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost_total =  (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)) # Formula for computing cost\n",
    "    cost = (1./m) * cost_total \n",
    "    \n",
    "    cost = np.squeeze(cost) # Help with the shape\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each parameter\n",
    "# Remember what hyperparameter is important for this step?\n",
    "# You will also find a useful, indexed value in the 'grads' dictionary created in backprop above\n",
    "# Replace False\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    parameters    -- dictionary with parameters \n",
    "    grads         -- dictionary with gradients (which function outputs this?)\n",
    "    learning_date -- step size to adjust parameters by\n",
    "    \n",
    "    Returns:\n",
    "    parameters    -- dictionary containing your updated parameters , same structure as original parameters dict\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # quotient of length of parameters divided by 2.\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to knit together everything you have done so far and allow for different layer sizes and lengths to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now stitch it all together. Essentially you will need to call all your functions in turn with the right arguments.\n",
    "# Initialise parameters\n",
    "# Undertake forward prop. What is our master function? Consider what we got from initialisation?\n",
    "# Undertake backwards prop. Again consider our master function for back prop.\n",
    "# Update parameters.\n",
    "# Replace False\n",
    "\n",
    "def total_backward_forward(X, Y, layers_dimensions, \n",
    "                           learning_rate, \n",
    "                           num_iterations, \n",
    "                           print_cost):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    X                 -- data\n",
    "    Y                 -- truth vector (1,0)'s\n",
    "    layers_dimensions -- list of dimensions for each layer of network\n",
    "    learning_rate     -- step size for gradient descent\n",
    "    num_iterations    -- number of training iterations to undertake\n",
    "    print_cost        -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    output:\n",
    "    parameters        -- parameters learnt by the model. Used to predict\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialise_parameters(layer_dimensions,\"he\")\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        AL, caches = total_forward(X, parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = total_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.438909\n",
      "Cost after iteration 100: 0.238204\n",
      "Cost after iteration 200: 0.223947\n",
      "Cost after iteration 300: 0.213048\n",
      "Cost after iteration 400: 0.199860\n",
      "Cost after iteration 500: 0.186238\n",
      "Cost after iteration 600: 0.175697\n",
      "Cost after iteration 700: 0.168282\n",
      "Cost after iteration 800: 0.161228\n",
      "Cost after iteration 900: 0.150834\n",
      "Cost after iteration 1000: 0.138049\n",
      "Cost after iteration 1100: 0.134548\n",
      "Cost after iteration 1200: 0.135313\n",
      "Cost after iteration 1300: 0.112429\n",
      "Cost after iteration 1400: 0.104182\n",
      "Cost after iteration 1500: 0.091920\n",
      "Cost after iteration 1600: 0.088201\n",
      "Cost after iteration 1700: 0.072190\n",
      "Cost after iteration 1800: 0.068768\n",
      "Cost after iteration 1900: 0.108205\n",
      "Cost after iteration 2000: 0.051812\n",
      "Cost after iteration 2100: 0.063090\n",
      "Cost after iteration 2200: 0.052246\n",
      "Cost after iteration 2300: 0.065222\n",
      "Cost after iteration 2400: 0.033629\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = total_backward_forward(X_train, \n",
    "                                    y_train, \n",
    "                                    layer_dimensions, \n",
    "                                    learning_rate = 0.01,\n",
    "                                    num_iterations = 2500, \n",
    "                                    print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict (Hold out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create your own predict function.\n",
    "# Note the number of training examples\n",
    "# Turn the probabilities into 0-1 predictions\n",
    "# Replace False\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    X           -- data (test set)\n",
    "    parameters  -- parameters of the trained model\n",
    "    \n",
    "    Output:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = y.shape[1] # How many training examples?\n",
    "    n = len(parameters) // 2\n",
    "    #p = np.zeros((1,m)) # Initialise probabilities to zero\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, caches = total_forward(X, parameters)\n",
    "    probas = probas[0]\n",
    "    \n",
    "    # convert probas to 0/1 predictions.\n",
    "    p = []\n",
    "    for i in range(0, len(probas)):\n",
    "        if probas[i] > 0.5:\n",
    "            p.append(1)\n",
    "        else:\n",
    "            p.append(0) \n",
    "    return p, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some predictions\n",
    "predictions, probas = predict(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10668ecc0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot of probabilities. Good check if something is wrong\n",
    "plt.scatter(range(len(probas)), probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1306\n",
       "1     155\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your prediction value counts\n",
    "pred_df = pd.DataFrame(predictions, columns=[\"prediction\"])\n",
    "pred_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of reshaping\n",
    "predictions_sk = np.reshape(predictions,(len(predictions), 1))\n",
    "\n",
    "y_test_sk = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix \n",
      " [[1248   49]\n",
      " [  58  106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1297\n",
      "           1       0.68      0.65      0.66       164\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1461\n",
      "   macro avg       0.82      0.80      0.81      1461\n",
      "weighted avg       0.93      0.93      0.93      1461\n",
      "\n",
      "Accuracy:  0.9267624914442163\n",
      "ROC_AUC:  0.9193448295315644\n"
     ]
    }
   ],
   "source": [
    "# Build some sklearn scores\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_sk), list(predictions_sk)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_sk, predictions_sk))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_sk, predictions_sk))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_sk, probas.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under the curve(AUC) is a better metric compared to Accuracy for our usecase.\n",
    "\n",
    "AUC calculation considers False positive (FP) cases while accuracy does not include them. As it is important to correctly classify objects that are not a car (False postives) and avoid security breaches through the basement car park AUC is a better evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension component:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the improving the model performance, I have tried different initialisation parameters, activation units and changing the structure of the neural network model.\n",
    "\n",
    "### Initialisation parameters:\n",
    "\n",
    "The inital weight matrix in each layer was multiplied with 0.01. After the neural network model was built with the default parameters the AUC score was at 0.60 and not able to correctly classify any of the car images. So, two different initialisation methods were experimented:\n",
    "\n",
    "#### Xavier initialisaion:\n",
    "\n",
    "Xavier initialisation is calculated by this formula multiplied by weight in each layer. size[l-1] denotes the number of units in the previous layer.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/j2pVTkHv/Screen-Shot-2019-04-22-at-8-25-52-pm.png\" style=\"height:100px\">\n",
    "\n",
    "#### He initialisation:\n",
    "\n",
    "He initialisation is calculated by this formula multiplied by weight in each layer. As in Xavier, size[l-1] denotes the number of units in the previous layer.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/0jJGTt28/Screen-Shot-2019-04-22-at-8-21-16-pm.png\" style=\"height:100px\">\n",
    "\n",
    "\n",
    "The AUC values after using 'Xavier' and 'He' initialisation were 0.86 and 0.87 respectively. These initialisations were better in classifying the car images accurately.\n",
    "\n",
    "\n",
    "### Activation units:\n",
    "\n",
    "The current neural network model used Relu units along with Sigmoid units in the last hiddent layer to perform Binomial classification. To further improve the model performance 'Tanh' and 'Leaky Relu' activation functions were experimented. These functions only replaced the 'Relu' activation unit and the 'Sigmoid' activation unit was kept constant. \n",
    "\n",
    "There was no improvement in performance compared to model with 'Relu' activation units.\n",
    "\n",
    "### Hidden layers:\n",
    "\n",
    "Number of hidden layers in the model were increased till 7 layers and the AUC started decreasing due to overfitting. It was found that the optimal number of hidden layers in the model was 4 layers.\n",
    "\n",
    "###  Units in the layers:\n",
    "\n",
    "The number of units in the hidden layers were increased and the optimal number of units in each layer was found. The final model had the structure of [3072, 40, 70, 90, 25, 1]\n",
    "\n",
    "3072 - Input features\n",
    "40 - Layer 1 units\n",
    "70 - Layer 2 units\n",
    "90 - Layer 3 units\n",
    "25 - Layer 4 units\n",
    "\n",
    "This model performed well with AUC of 0.908.\n",
    "\n",
    "### Learning rate:\n",
    "\n",
    "The initial learning rate was set to 0.01. It was adjusted to 0.1 and 0.001 for see if the performance of the model improved. But, these changed only reduced the AUC score and the changes were discarded.\n",
    "\n",
    "### Number of iterations:\n",
    "\n",
    "The initial number of irterations was set to 1500. The AUC score kept on improving for increments of the iterations until it started decreasing once it reached 3000 iterations. So, The final model was concluded at 2500 iteration with a AUC score of 0.919.\n",
    "\n",
    "<br/>True positives = 86\n",
    "<br/>True negatives = 1247\n",
    "<br/>False positives = 24\n",
    "<br/>False negatives = 78\n",
    "<br/>\n",
    "<br/>The final model had the specification:\n",
    "\n",
    "\n",
    "<b>Network structure</b> - [3072, 40, 70, 90, 25, 1]\n",
    "<br/>\n",
    "<b>Initialisation method</b> - He\n",
    "<br/>\n",
    "<b>Activation unit</b>- Relu\n",
    "<br/>\n",
    "<b>Learning rate</b> - 0.01\n",
    "<br/>\n",
    "<b>Number of iterations</b> - 2500\n",
    "\n",
    "Further details of the model evaluation can be found in the attached link.(https://drive.google.com/file/d/1VPg62ReSo3GFYxFyn4lq4aZlfxYjjAv9/view?usp=sharing)\n",
    "\n",
    "If this algorithm is successfully deployed in the motion activated camers for automatic lifting of the gate upon arrival of an automobile, it would operate accurately in ~92% of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
